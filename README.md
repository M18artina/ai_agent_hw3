
# Reinforcement Learning Agent â€“ GridWorld + Q-learning

Tento projekt ukazuje jednoduchÃ©ho agenta v prostÅ™edÃ­ `GridWorld`, kterÃ½ se pomocÃ­ Q-learningu uÄÃ­ nachÃ¡zet cestu k cÃ­li. CÃ­lovÃ¡ pozice se v kaÅ¾dÃ© epizodÄ› mÄ›nÃ­, coÅ¾ nutÃ­ agenta generalizovat nauÄenÃ© chovÃ¡nÃ­.

## ğŸ”§ PouÅ¾itÃ© soubory

- `environment.py` â€“ vlastnÃ­ RL prostÅ™edÃ­ (mÅ™Ã­Å¾ka s dynamickÃ½m cÃ­lem)
- `agent.py` â€“ tabulkovÃ½ Q-learning agent
- `train.py` â€“ hlavnÃ­ skript pro trÃ©nink, test a vizualizaci vÃ½sledkÅ¯

## ğŸ§  Co agent umÃ­

- NajÃ­t nejkratÅ¡Ã­ cestu z vÃ½chozÃ­ pozice `(0, 0)` do nÃ¡hodnÃ©ho cÃ­le
- PÅ™izpÅ¯sobit se mÄ›nÃ­cÃ­m podmÃ­nkÃ¡m
- UÄit se z odmÄ›ny a penalizace
- Vizualizovat â€jistotuâ€œ rozhodnutÃ­ pomocÃ­ heatmapy Q-hodnot

## â–¶ï¸ SpuÅ¡tÄ›nÃ­

1. Aktivuj virtuÃ¡lnÃ­ prostÅ™edÃ­ (pokud mÃ¡Å¡):

```
source .venv/bin/activate        # macOS/Linux
.\.venv\Scripts ctivate         # Windows
```

2. Nainstaluj zÃ¡vislosti:

```
pip install -r requirements.txt
```

3. SpusÅ¥ skript:

```
python train.py
```

## ğŸ“Š VÃ½stupy

- **Heatmapa Q-hodnot** â€“ ukazuje, kde mÃ¡ agent vysokou dÅ¯vÄ›ru v rozhodnutÃ­
- **Test po trÃ©ninku** â€“ ukazuje, jak agent reaguje bez nÃ¡hodnosti
- **VÃ½pis nejlepÅ¡Ã­ trajektorie** â€“ nejkratÅ¡Ã­ cesta k libovolnÃ©mu cÃ­li
- **VÃ½pis nejlepÅ¡Ã­ch odmÄ›n** â€“ pro kaÅ¾dou cÃ­lovou pozici

## âœ… PÅ™Ã­klad vÃ½stupu

```
Epizoda 500: odmÄ›na = 0.93

Agent dosÃ¡hl cÃ­le za 8 krokÅ¯!

NejkratÅ¡Ã­ nalezenÃ¡ trajektorie (1 krokÅ¯):
  1. (0, 0)
  2. (1, 0)

NejlepÅ¡Ã­ dosaÅ¾enÃ© odmÄ›ny podle cÃ­lovÃ½ch pozic:
  CÃ­l (0, 1): odmÄ›na = 1
  CÃ­l (4, 4): odmÄ›na = 0.88
  ...
```

## ğŸ“ DoporuÄenÃ¡ struktura repozitÃ¡Å™e

```
reinforcement_agent/
â”œâ”€â”€ agent.py
â”œâ”€â”€ environment.py
â”œâ”€â”€ train.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ“ PoznÃ¡mka

Projekt je ideÃ¡lnÃ­ pro vÃ½uku principÅ¯:
- Reinforcement Learningu
- Q-learningu
- generalizace v dynamickÃ©m prostÅ™edÃ­
